import os
import re
import csv
import time
import json
import torch
import warnings
import numpy as np
from bs4 import BeautifulSoup
from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, set_seed
from torch.nn.parallel import DataParallel
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Process, Queue, cpu_count, set_start_method
from tqdm.auto import tqdm  # tqdm.autoë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë‚˜ì€ ì§„í–‰ ìƒí™© í‘œì‹œ
import threading
import requests
import random
import subprocess
import atexit
from raptor.RetrievalAugmentation import RetrievalAugmentation, RetrievalAugmentationConfig
from raptor.Custom import CustomSummarizationModel, CustomQAModel, CustomEmbeddingModel
# í† í¬ë‚˜ì´ì € ë³‘ë ¬í™” ì„¤ì •
os.environ["emb_TOKENIZERS_PARALLELISM"] = "false"

# ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ì‹ ì„¤ì •
try:
    set_start_method('spawn')
except RuntimeError:
    pass  # ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆëŠ” ê²½ìš° ë¬´ì‹œ

# ê³µí†µ ì„¤ì •
ROOT_DIR = "data"
TOP_K = 5

# Prompt í…œí”Œë¦¿ íŒŒì¼ ê²½ë¡œ
PROMPT_DIR = os.path.join(ROOT_DIR, "prompt")
PROMPT_LLM_FILTERING = os.path.join(PROMPT_DIR, "mydata_llm_filtering.txt")
PROMPT_LLM_FILTERING_NP = os.path.join(PROMPT_DIR, "mydata_llm_filtering_np.txt")
PROMPT_LLM_FILTERING_WOKEEP = os.path.join(PROMPT_DIR, "mydata_llm_filtering_wokeep.txt")
PROMPT_LLM_FILTERING_WOSUM = os.path.join(PROMPT_DIR, "mydata_llm_filtering_wosum.txt")
PROMPT_LLM_FILTERING_P_NP = os.path.join(PROMPT_DIR, "mydata_llm_filtering_p_np.txt")
PROMPT_LLM_FILTERING_SYSTEM = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt.txt")
PROMPT_LLM_FILTERING_SYSTEM_EX = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_ex.txt")
PROMPT_LLM_FILTERING_USER = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt.txt")
PROMPT_LLM_SUMMARIZING = os.path.join(PROMPT_DIR, "mydata_llm_summarizing.txt")
PROMPT_LLM_SUMMARIZING_ONLY = os.path.join(PROMPT_DIR, "mydata_llm_summarizing_only_improved.txt")  # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
PROMPT_GENERATION = os.path.join(PROMPT_DIR, "mydata_generation.txt")
PROMPT_LLM_SUMMARIZING_ONLY_WOPREF = os.path.join(PROMPT_DIR, "mydata_llm_summarizing_only_wopref.txt")
PROMPT_LLM_SUMMARIZING_ENHANCED = os.path.join(PROMPT_DIR, "summarizing_cm_improved.txt")
PROMPT_LLM_SUMMARIZING_SYSTEM = os.path.join(PROMPT_DIR, "enhanced/enhanced_sum_systemprompt.txt")
PROMPT_LLM_SUMMARIZING_SYSTEM_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_sum_systemprompt_ld.txt")
PROMPT_LLM_SUMMARIZING_USER = os.path.join(PROMPT_DIR, "enhanced/enhanced_sum_userprompt.txt")
PROMPT_LLM_SUMMARIZING_USER_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_sum_userprompt_ld.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOSUM = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wosum.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOSUM_EX = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wosum_ex.txt")
PROMPT_LLM_FILTERING_SYSTEM_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_ld.txt")
PROMPT_LLM_FILTERING_USER_WOSUM = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_wosum.txt")
PROMPT_LLM_FILTERING_USER_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_ld.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOKEEP = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wokeep.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOKEEP_EX = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wokeep_ex.txt")
PROMPT_LLM_FILTERING_USER_WOKEEP = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_wokeep.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOFILTER = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wofilter.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOFILTER_EX = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wofilter_ex.txt")
PROMPT_LLM_FILTERING_USER_WOFILTER = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_wofilter.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOKEEP_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wokeep_ld.txt")
PROMPT_LLM_FILTERING_USER_WOKEEP_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_wokeep_ld.txt")
PROMPT_LLM_FILTERING_SYSTEM_EXPLICIT_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_explicit_ld.txt")
PROMPT_LLM_FILTERING_USER_EXPLICIT_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_explicit_ld.txt")
PROMPT_LLM_FILTERING_SYSTEM_WOKEEP_EXPLICIT_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_systemprompt_wokeep_explicit_ld.txt")
PROMPT_LLM_FILTERING_USER_WOKEEP_EXPLICIT_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_userprompt_wokeep_explicit_ld.txt")
PROMPT_LLM_SUMMARIZING_SYSTEM_EXPLICIT_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_sum_systemprompt_explicit_ld.txt")
PROMPT_LLM_SUMMARIZING_USER_EXPLICIT_LD = os.path.join(PROMPT_DIR, "enhanced/enhanced_sum_userprompt_explicit_ld.txt")

PROMPT_LLM_WQ_SYSTEM_PROMPT = os.path.join(PROMPT_DIR, "enhanced/wq_select_systemprompt.txt")
PROMPT_LLM_WQ_USER_PROMPT = os.path.join(PROMPT_DIR, "enhanced/wq_select_userprompt.txt")

PROMPT_LLM_WQ_CHECK_SYSTEM_PROMPT = os.path.join(PROMPT_DIR, "enhanced/wq_check_systemprompt.txt")
PROMPT_LLM_WQ_CHECK_USER_PROMPT = os.path.join(PROMPT_DIR, "enhanced/wq_check_userprompt.txt")


# Indexing ê´€ë ¨ ì„¤ì •
INDEXING_REPORT_FILE = "indexing_report.csv"
THRESHOLD = 0.45

# Generation ê´€ë ¨ ì„¤ì •
GENERATION_REPORT_FILE = "generation_report.csv"

# Evaluation ê´€ë ¨ ì„¤ì •
EVALUATION_REPORT_FILE = "evaluation_report.csv"
ERROR_TYPE_DIR = os.path.join(ROOT_DIR, "error_type")

def set_global_seed(seed):
    """ëª¨ë“  ëœë¤ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    set_seed(seed)

class MyDataUtils:
    # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì‹œë“œ ì„¤ì •
    _seed = 42
    set_global_seed(_seed)
    
    def __init__(self, mode, method, device, use_multi_gpu, chunk_mode, output_dir, persona_task_file=None, emb_model_name="facebook/contriever", doc_mode="sample", vllm_server_url="http://localhost:8008/v1", llm_model_name="meta-llama/Llama-3.1-8B-Instruct", index_type="hnsw", ssh_config=None, use_trust_align=False):
        """
        MyData ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            mode (str): ì‹¤í–‰ ëª¨ë“œ ('standard' ë˜ëŠ” 'persona')
            method (str): ì‚¬ìš©í•  ë°©ë²• ('standard', 'naive_p', 'cosine_only', 'random', 'hipporag')
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
            use_multi_gpu (bool): ë©€í‹° GPU ì‚¬ìš© ì—¬ë¶€
            chunk_mode (str): ì²­í¬ ëª¨ë“œ ('wdoc' ë˜ëŠ” 'wodoc')
            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            persona_task_file (str, optional): persona task íŒŒì¼ ê²½ë¡œ
            emb_model_name (str): ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            doc_mode (str): ë¬¸ì„œ ëª¨ë“œ ('total' ë˜ëŠ” 'sample')
            vllm_server_url (str): vLLM ì„œë²„ URL
            llm_model_name (str): LLM ëª¨ë¸ ì´ë¦„
            index_type (str): FAISS ì¸ë±ìŠ¤ íƒ€ì… ('flat' ë˜ëŠ” 'hnsw')
            ssh_config (dict, optional): SSH ì„¤ì • {'host': 'hostname', 'user': 'username', 'port': 22, 'key_file': '/path/to/key'}
        """
        self.mode = mode
        self.method = method
        if self.method == "raptor":
            custom_summarizer = CustomSummarizationModel()
            custom_qa = CustomQAModel()
            custom_embedding = CustomEmbeddingModel()
            custom_config = RetrievalAugmentationConfig(
                summarization_model=custom_summarizer,
                qa_model=custom_qa,
                embedding_model=custom_embedding,
            )
            self.raptor = RetrievalAugmentation(config=custom_config)
            self.raptor_save = os.path.join(ROOT_DIR, "corpus/raptor_tree")
        self.device = device
        self.use_multi_gpu = use_multi_gpu
        self.chunk_mode = chunk_mode
        self.emb_model_name = emb_model_name
        self.doc_mode = doc_mode
        self.vllm_server_url = vllm_server_url
        self.llm_model_name = llm_model_name
        self.index_type = index_type
        self.ssh_config = ssh_config
        self.ssh_tunnel_process = None
        self.use_trust_align = use_trust_align
        # SSH ì„¤ì •ì„ ì´ìš©í•œ ëª¨ë¸ë³„ ì„œë²„ URL ì„¤ì •
        if ssh_config:
            model_config = None
            
            # ëª¨ë¸ë³„ ì„¤ì •ì´ ìˆëŠ” ê²½ìš°
            if "models" in ssh_config and llm_model_name in ssh_config["models"]:
                model_config = ssh_config["models"][llm_model_name]
                print(f"ğŸ”— Found SSH config for model: {llm_model_name}")
            # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            elif "default" in ssh_config:
                model_config = ssh_config["default"]
                print(f"ğŸ”— Using default SSH config for model: {llm_model_name}")
            # ë‹¨ì¼ ì„¤ì • (ì´ì „ ë²„ì „ í˜¸í™˜ì„±)
            elif "host" in ssh_config:
                model_config = ssh_config
                print(f"ğŸ”— Using single SSH config for model: {llm_model_name}")
            
            if model_config:
                local_port = model_config.get("local_port", 8009)
                self.vllm_server_url = f"http://localhost:{local_port}/v1"
                print(f"ğŸ”— Using SSH-forwarded server: {self.vllm_server_url}")
                
                # SSH í„°ë„ë§ ìë™ ì„¤ì •
                self.setup_ssh_tunnel(model_config)
            else:
                self.vllm_server_url = vllm_server_url
                print(f"ğŸŒ No SSH config found, using standard server: {self.vllm_server_url}")
        else:
            self.vllm_server_url = vllm_server_url
            print(f"ğŸŒ Using standard server for {llm_model_name}: {self.vllm_server_url}")
        
        # doc_modeì™€ chunk_modeì— ë”°ë¥¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        model_name_clean = emb_model_name.replace("/", "_")
        
        if doc_mode == "total":
            self.chunk_file = "data/corpus/full_chunks_with_doc.jsonl"
            self.embedding_file = f"data/corpus/full_embeddings_with_doc_{model_name_clean}.npy"
        elif doc_mode == "total_sw":
            self.chunk_file = "data/corpus/full_chunks_with_doc_sw.jsonl"
            self.embedding_file = f"data/corpus/full_sw_embeddings_with_doc_{model_name_clean}.npy"
        elif doc_mode == "sample_sw":
            self.chunk_file = "data/corpus/sampled_chunks_with_doc_sw.jsonl"
            self.embedding_file = f"data/corpus/sampled_sw_embeddings_with_doc_{model_name_clean}.npy"
        else:  # sample
            if chunk_mode == "wodoc":
                self.chunk_file = "data/corpus/sampled_chunks.jsonl"
                self.embedding_file = f"data/corpus/sampled_embeddings_{model_name_clean}.npy"
            elif chunk_mode == "wdoc":
                self.chunk_file = "data/corpus/sampled_chunks_with_doc.jsonl"
                self.embedding_file = f"data/corpus/sampled_embeddings_with_doc_{model_name_clean}.npy"
            else:
                raise ValueError(f"Invalid chunk_mode: {chunk_mode}. Must be either 'wodoc' or 'wdoc'")
        
        self.persona_task_file = f"data/{persona_task_file}"
        print(f"Persona task file: {self.persona_task_file}")
        
        # doc_modeì— ë”°ë¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if doc_mode == "total":
            self.output_dir = f"{output_dir}/total"
        elif doc_mode == "total_sw":
            self.output_dir = f"{output_dir}/total_sw"
        elif doc_mode == "sample_sw":
            self.output_dir = f"{output_dir}/sample_sw"
        else:  # sample
            self.output_dir = f"{output_dir}/sample"
        self.emb_tokenizer = None
        self.emb_model = None
        self.hf_emb_tokenizer = None
        self.hf_model = None
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
        if self.use_multi_gpu and torch.cuda.device_count() > 1:
            self.num_gpus = torch.cuda.device_count()
            # ê° GPUì˜ ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            if gpu_memory >= 80e9:  # 80GB
                self.base_batch_size = 64
            elif gpu_memory >= 48e9:  # 48GB
                self.base_batch_size = 128
            elif gpu_memory >= 24e9:  # 24GB
                self.base_batch_size = 64
            elif gpu_memory >= 16e9:  # 16GB
                self.base_batch_size = 32
            else:
                self.base_batch_size = 16
            self.batch_size = self.base_batch_size * self.num_gpus
        else:
            self.num_gpus = 1
            # ë‹¨ì¼ GPUì˜ ê²½ìš°ì—ë„ ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            if gpu_memory >= 80e9:  # 80GB
                self.batch_size = 256
            elif gpu_memory >= 48e9:  # 48GB
                self.batch_size = 128
            elif gpu_memory >= 24e9:  # 24GB
                self.batch_size = 64
            elif gpu_memory >= 16e9:  # 16GB
                self.batch_size = 32
            else:
                self.batch_size = 16
    
    def setup_ssh_tunnel(self, ssh_config):
        """SSH í„°ë„ë§ ì„¤ì •"""
        try:
            host = ssh_config.get('host', 'localhost')
            user = ssh_config.get('user', 'root')
            port = ssh_config.get('port', 22)
            key_file = ssh_config.get('key_file', None)
            remote_port = ssh_config.get('remote_port', 8009)
            local_port = ssh_config.get('local_port', 8009)
            
            # SSH ì„¤ì • ì •ë³´ ì¶œë ¥
            print(f"ğŸ”— SSH ì„¤ì • ì •ë³´:")
            print(f"   Host: {host}")
            print(f"   User: {user}")
            print(f"   Port: {port}")
            print(f"   Key file: {key_file}")
            print(f"   Remote port: {remote_port}")
            print(f"   Local port: {local_port}")
            
            # í‹¸ë“œ í™•ì¥ ì²˜ë¦¬
            if key_file:
                if key_file.startswith('~'):
                    key_file = os.path.expanduser(key_file)
                elif key_file.startswith('./'):
                    key_file = os.path.abspath(key_file)
                
                print(f"   Expanded key file path: {key_file}")
            
            # SSH í‚¤ íŒŒì¼ ì¡´ì¬ ë° ê¶Œí•œ í™•ì¸
            if key_file:
                if not os.path.exists(key_file):
                    raise FileNotFoundError(f"SSH key file not found: {key_file}")
                
                # í‚¤ íŒŒì¼ ê¶Œí•œ í™•ì¸
                file_stat = os.stat(key_file)
                file_mode = oct(file_stat.st_mode)[-3:]
                if file_mode != '600':
                    print(f"âš ï¸ SSH key file permissions: {file_mode} (recommended: 600)")
                    print(f"   Run: chmod 600 {key_file}")
                    # ê¶Œí•œ ìë™ ìˆ˜ì • ì‹œë„
                    try:
                        os.chmod(key_file, 0o600)
                        print(f"âœ… SSH key file permissions fixed to 600")
                    except Exception as e:
                        print(f"âŒ Failed to fix permissions: {e}")
                        raise
            
            print(f"ğŸ”— Setting up SSH tunnel...")
            print(f"   Local port: {local_port}")
            print(f"   Remote: {user}@{host}:{port}")
            print(f"   Remote port: {remote_port}")
            
            # ê¸°ì¡´ í„°ë„ ì¢…ë£Œ (í¬íŠ¸ ì¶©ëŒ ë°©ì§€)
            self.cleanup_existing_tunnel(local_port)
            
            # SSH ì—°ê²° í…ŒìŠ¤íŠ¸
            print(f"ğŸ”— Testing SSH connection...")
            test_cmd = ['ssh', '-o', 'ConnectTimeout=10', '-o', 'StrictHostKeyChecking=no', '-p', str(port)]
            if key_file:
                test_cmd.extend(['-i', key_file])
            test_cmd.extend([f'{user}@{host}', 'echo "SSH connection test successful"'])
            
            try:
                result = subprocess.run(test_cmd, capture_output=True, text=True, timeout=15)
                if result.returncode == 0:
                    print(f"âœ… SSH connection test passed")
                else:
                    print(f"âŒ SSH connection test failed:")
                    print(f"   stdout: {result.stdout}")
                    print(f"   stderr: {result.stderr}")
                    raise RuntimeError(f"SSH connection test failed: {result.stderr}")
            except subprocess.TimeoutExpired:
                raise RuntimeError("SSH connection test timed out")
            except Exception as e:
                raise RuntimeError(f"SSH connection test error: {e}")
            
            # SSH ëª…ë ¹ êµ¬ì„± (evaluation_with_different_llm.py ë°©ì‹)
            ssh_cmd = [
                "ssh",
                "-i", key_file,
                "-L", f"{local_port}:localhost:{remote_port}",
                "-N",  # ëª…ë ¹ì–´ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
                "-f",  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
                f"{user}@{host}",
                "-o", "StrictHostKeyChecking=no",
                "-o", "UserKnownHostsFile=/dev/null"
            ]
            
            if port != 22:
                ssh_cmd.extend(["-p", str(port)])
            
            print(f"ğŸ”— Starting SSH tunnel...")
            print(f"   Local port: {local_port} -> Remote port: {remote_port}")
            print(f"   Command: {' '.join(ssh_cmd)}")
            
            # SSH í„°ë„ë§ ì‹¤í–‰
            self.ssh_tunnel_process = subprocess.Popen(
                ssh_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # í„°ë„ ì„¤ì • ëŒ€ê¸° (evaluation_with_different_llm.pyì™€ ë™ì¼)
            time.sleep(3)
            
            # SSH í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸ (evaluation_with_different_llm.py ë°©ì‹)
            if self.ssh_tunnel_process.poll() is None:
                print("âœ… SSH tunnel process started successfully")
                
                # í¬íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸
                print(f"ğŸ”— Testing local port connection on {local_port}...")
                port_test_attempts = 0
                max_port_attempts = 5
                
                while port_test_attempts < max_port_attempts:
                    if self.test_local_port(local_port):
                        print(f"âœ… SSH tunnel established successfully on port {local_port}")
                        break
                    else:
                        port_test_attempts += 1
                        if port_test_attempts < max_port_attempts:
                            print(f"â³ Port test attempt {port_test_attempts}/{max_port_attempts}, waiting...")
                            time.sleep(2)
                        else:
                            print(f"âš ï¸ Port {local_port} not responding after {max_port_attempts} attempts")
                            print("   SSH tunnel may need more time or there might be an issue")
                            # ê³„ì† ì§„í–‰ (í„°ë„ì´ ëŠ¦ê²Œ í™œì„±í™”ë  ìˆ˜ ìˆìŒ)
                
            else:
                # í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œëœ ê²½ìš° (evaluation_with_different_llm.pyì—ì„œëŠ” ì´ ê²½ìš°ë¥¼ ì—ëŸ¬ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
                stdout, stderr = self.ssh_tunnel_process.communicate()
                error_msg = stderr.decode('utf-8') if stderr else ""
                stdout_msg = stdout.decode('utf-8') if stdout else ""
                
                print(f"âš ï¸ SSH tunnel process terminated immediately:")
                print(f"   Exit code: {self.ssh_tunnel_process.returncode}")
                print(f"   stdout: {stdout_msg}")
                print(f"   stderr: {error_msg}")
                
                # Warning ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì •ìƒìœ¼ë¡œ ì²˜ë¦¬ (known_hosts ì¶”ê°€ ë©”ì‹œì§€)
                if "Warning" in error_msg and "known hosts" in error_msg and self.ssh_tunnel_process.returncode == 0:
                    print("   This appears to be just a known hosts warning, checking if tunnel is actually working...")
                    
                    # ì‹¤ì œë¡œ í¬íŠ¸ê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    time.sleep(2)
                    if self.test_local_port(local_port):
                        print(f"âœ… SSH tunnel is actually working on port {local_port}")
                        self.ssh_tunnel_process = None  # í”„ë¡œì„¸ìŠ¤ ì¶”ì  ë¹„í™œì„±í™”
                    else:
                        print("âš ï¸ SSH tunnel not responding on expected port")
                        print("   This might be normal - some tunnels take time to establish")
                        print("   Continuing with the assumption that tunnel will work...")
                        self.ssh_tunnel_process = None  # í”„ë¡œì„¸ìŠ¤ ì¶”ì  ë¹„í™œì„±í™”
                else:
                    raise RuntimeError(f"SSH tunnel failed to establish: {error_msg}")
            
            # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ í„°ë„ ì •ë¦¬
            atexit.register(self.cleanup_ssh_tunnel)
            
        except Exception as e:
            print(f"âŒ Failed to setup SSH tunnel: {str(e)}")
            print(f"   Please check:")
            print(f"   1. SSH key file exists and has correct permissions (600): {key_file}")
            print(f"   2. Server is accessible: {host}:{port}")
            print(f"   3. Remote port is open: {remote_port}")
            print(f"   4. SSH connection works: ssh -i {key_file} {user}@{host}")
            raise
    
    def cleanup_existing_tunnel(self, local_port):
        """ê¸°ì¡´ í„°ë„ ì •ë¦¬ (í¬íŠ¸ ì¶©ëŒ ë°©ì§€)"""
        try:
            # lsof ëª…ë ¹ìœ¼ë¡œ í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
            result = subprocess.run(
                ['lsof', '-ti', f':{local_port}'],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0 and result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    try:
                        subprocess.run(['kill', '-9', pid], check=True)
                        print(f"ğŸ”— Killed existing process on port {local_port}: PID {pid}")
                    except subprocess.CalledProcessError:
                        pass
                        
        except FileNotFoundError:
            # lsof ëª…ë ¹ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
            pass
        except Exception as e:
            print(f"âš ï¸ Error cleaning up existing tunnel: {e}")
    
    def test_local_port(self, port):
        """ë¡œì»¬ í¬íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            return result == 0
        except Exception:
            return False
    
    def cleanup_ssh_tunnel(self):
        """SSH í„°ë„ ì •ë¦¬"""
        if self.ssh_tunnel_process:
            try:
                self.ssh_tunnel_process.terminate()
                self.ssh_tunnel_process.wait(timeout=5)
                print("ğŸ”— SSH tunnel closed")
            except subprocess.TimeoutExpired:
                self.ssh_tunnel_process.kill()
                print("ğŸ”— SSH tunnel forcibly closed")
            except Exception as e:
                print(f"âš ï¸ Error closing SSH tunnel: {str(e)}")
    
    def __del__(self):
        """ì†Œë©¸ìì—ì„œ SSH í„°ë„ ì •ë¦¬"""
        self.cleanup_ssh_tunnel()
    
    def load_models(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"Loading {self.emb_model_name} model...")
        self.emb_tokenizer = AutoTokenizer.from_pretrained(self.emb_model_name)
        self.emb_model = AutoModel.from_pretrained(self.emb_model_name).eval()
        
        if self.use_multi_gpu and torch.cuda.device_count() > 1:
            print(f"Using {self.num_gpus} GPUs with batch size {self.batch_size}!")
            # ë¨¼ì € ëª¨ë¸ì„ deviceë¡œ ì´ë™
            self.emb_model = self.emb_model.to(self.device)
            # ê·¸ ë‹¤ìŒ DataParallel ì ìš©
            self.emb_model = DataParallel(
                self.emb_model,
                device_ids=list(range(self.num_gpus)),
                output_device=0,  # ë©”ì¸ GPU
                dim=0  # ë°°ì¹˜ ì°¨ì›
            )
            print(f"Embedding model distributed across {self.num_gpus} GPUs")
        else:
            self.emb_model = self.emb_model.to(self.device)
            print(f"Embedding model loaded on {self.device}")
        
        # LLM ëª¨ë¸ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
        if self.method in ["naive_p"] and self.mode in [""]:
            print("Loading LLM model...")
            set_seed(42)
            self.hf_emb_tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)
            self.hf_emb_tokenizer.padding_side = 'left'  # ì™¼ìª½ íŒ¨ë”©ìœ¼ë¡œ ì„¤ì •
            self.hf_emb_tokenizer.pad_token = self.hf_emb_tokenizer.eos_token
            
            # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
            torch.cuda.empty_cache()
            if self.use_multi_gpu and torch.cuda.device_count() > 1:
                # ê° GPUì— ëª¨ë¸ì„ ë¶„ì‚°í•˜ì—¬ ë¡œë“œ
                self.hf_model = AutoModelForCausalLM.from_pretrained(
                    self.llm_model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",  # ìë™ìœ¼ë¡œ GPUì— ë¶„ì‚°
                    use_cache=True,     # ìºì‹œ ì‚¬ìš©
                    low_cpu_mem_usage=True  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”
                )
                print(f"LLM model automatically distributed across {torch.cuda.device_count()} GPUs")
            else:
                self.hf_model = AutoModelForCausalLM.from_pretrained(
                    self.llm_model_name,
                    torch_dtype=torch.float16,
                    use_cache=True,
                    low_cpu_mem_usage=True
                ).to(self.device)
                print(f"LLM model loaded on {self.device}")
            self.hf_model.eval()
    
    def embed_texts(self, texts):
        all_embs = []
        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™”
        batch_size = self.batch_size
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            inputs = self.emb_tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.emb_model(**inputs)
                if isinstance(outputs.last_hidden_state, tuple):
                    embeddings = outputs.last_hidden_state[0][:, 0, :].cpu().numpy()
                else:
                    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                all_embs.append(embeddings)
        return np.vstack(all_embs)
    
    def embed_query(self, query):
        inputs = self.emb_tokenizer(query, return_tensors="pt", truncation=True, padding=True).to(self.device)
        with torch.no_grad():
            outputs = self.emb_model(**inputs)
            if isinstance(outputs.last_hidden_state, tuple):
                query_emb = outputs.last_hidden_state[0][:, 0, :].cpu().numpy()
            else:
                query_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)
        return query_emb
    
    def call_llm(self, prompts, max_new_tokens=512):
        """LLM í˜¸ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            results = []
            for i in range(0, len(prompts), self.batch_size):
                batch_prompts = prompts[i:i + self.batch_size]
                with warnings.catch_warnings():
                    warnings.filterwarnings(
                        "ignore",
                        message="Starting from v4.46, the `logits` model output will have the same type as the model",
                        category=UserWarning,
                    )
                    # ë°°ì¹˜ ì „ì²´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
                    formatted_prompts = [
                        self.hf_emb_tokenizer.apply_chat_template(
                            [{"role": "user", "content": prompt}],
                            tokenize=False,
                            add_generation_prompt=True
                        ) for prompt in batch_prompts
                    ]
                    
                    # ë°°ì¹˜ ì…ë ¥ ì¤€ë¹„
                    inputs = self.hf_emb_tokenizer(
                        formatted_prompts,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=2048
                    ).to(self.device)
                    
                    # ìƒì„± ì„¤ì •
                    generation_config = {
                        "max_new_tokens": max_new_tokens,
                        "do_sample": True,
                        "temperature": 0.1,
                        "pad_token_id": self.hf_emb_tokenizer.eos_token_id,
                        "eos_token_id": self.hf_emb_tokenizer.eos_token_id
                    }
                    
                    # ë°°ì¹˜ ìƒì„± (ê²½ê³  ë©”ì‹œì§€ ì œê±°)
                    with torch.no_grad():
                        outputs = self.hf_model.generate(
                            **inputs,
                            **generation_config
                        )
                    
                    # ë°°ì¹˜ ê²°ê³¼ ë””ì½”ë”©
                    batch_results = [
                        self.hf_emb_tokenizer.decode(output, skip_special_tokens=True)
                        for output in outputs
                    ]
                    results.extend(batch_results)
            
            return results
            
        except Exception as e:
            print(f"Error in call_llm: {str(e)}")
            return None

    def generate_message_vllm(self, messages, system_prompt, max_tokens=512):
        headers = {"Content-Type": "application/json"}
        endpoint = f"{self.vllm_server_url}/chat/completions"
        
        # vLLM ì„œë²„ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ë©”ì‹œì§€ í˜•ì‹ ìˆ˜ì •
        formatted_messages = []
        if system_prompt:
            formatted_messages.append({"role": "system", "content": system_prompt})
        formatted_messages.extend(messages)
        
        payload = {
            "model": self.llm_model_name,
            "messages": formatted_messages,
            "temperature": 0.0,
            "max_tokens": max_tokens,
            "seed": 42,
            "top_p": 1.0,
            "top_k": -1,  # top_k ë¹„í™œì„±í™”
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stream": False,
            "dtype": "float32"
        }
        
        for attempt in range(5):
            try:
                # timeoutì„ 60ì´ˆë¡œ ëŠ˜ë¦¼
                response = requests.post(endpoint, headers=headers, json=payload, timeout=60)
                if response.status_code != 200:
                    print(f"Error response: {response.text}")
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except requests.exceptions.RequestException as e:
                print(f"[Attempt {attempt+1}/5] Request failed: {e}")
                if attempt < 4:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
                    # ì¬ì‹œë„ ê°„ê²©ì„ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ (1ì´ˆ, 2ì´ˆ, 4ì´ˆ, 8ì´ˆ)
                    wait_time = min(2 ** attempt, 10)
                    print(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
        raise RuntimeError("Failed to get response from vLLM server after 5 attempts")

    def parse_explanation_and_answer(self, input_string):
        soup = BeautifulSoup(input_string, "html.parser")
        explanation_tag = soup.find("explanation")
        explanation = explanation_tag.text.strip() if explanation_tag else ""
        answer_tag = soup.find("answer")
        answer = answer_tag.text.strip() if answer_tag else ""
        return explanation, answer

    def parse_preference_and_answer(self, input_string):
        soup = BeautifulSoup(input_string, "html.parser")
        preference_tag = soup.find("preference")
        preference = preference_tag.text.strip() if preference_tag else ""
        answer_tag = soup.find("answer")
        answer = answer_tag.text.strip() if answer_tag else ""
        return preference, answer
    
    def load_persona_data(self, persona_index):
        with open(self.persona_task_file, "r", encoding="utf-8") as f:
            personas = json.load(f)
        return next(p for p in personas if p["persona_index"] == persona_index)

    def load_persona_questions(self, file_path, persona_index):
        with open(file_path, "r", encoding="utf-8") as f:
            personas = json.load(f)
        for p in personas:
            if p["persona_index"] == persona_index:
                all_qs = []
                for block in p["preference_blocks"]:
                    pref = block["preference"]
                    for q in block["queries"]:
                        all_qs.append((pref, q["question"]))
                return all_qs
        raise ValueError(f"Persona index {persona_index} not found.")

    def retrieve_top_k(self, query, index, chunks, top_k=TOP_K):
        query_emb = self.embed_query(query)
        start_retrieval = time.time()
        D, I = index.search(query_emb, top_k)
        retrieval_time = time.time() - start_retrieval
        return [chunks[i] for i in I[0]], retrieval_time

    def retrieve_top_k_wq(self, query, preference, index, chunks, top_k=TOP_K):
        query_emb = self.embed_query(query)
        preference_emb = self.embed_query(preference)
        query_emb = query_emb + preference_emb
        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)    # !
        start_retrieval = time.time()
        D, I = index.search(query_emb, top_k)
        retrieval_time = time.time() - start_retrieval
        return [chunks[i] for i in I[0]], retrieval_time
    
    def extract_preferences_from_response(self, response):
        soup = BeautifulSoup(response, "html.parser")
        preferences = [tag.text.strip() for tag in soup.find_all("preference")]
        return preferences

    def retrieve_top_k_wq_llm(self, query, preferences, index, chunks, top_k=TOP_K):
        system_prompt = self.load_prompt_template(PROMPT_LLM_WQ_SYSTEM_PROMPT)
        preferences_text = "\n".join(f" - {p}" for p in preferences)
        # preferences_text = "\n".join(f"{i + 1}. {p}" for i, p in enumerate(preferences))
        filled_user_prompt = self.load_prompt_template(PROMPT_LLM_WQ_USER_PROMPT).format(preferences=preferences_text, question=query)
        response = self.generate_message_vllm(
            messages=[{"role": "user", "content": filled_user_prompt}],
            system_prompt=system_prompt,
            max_tokens=1024
        )
        
        query_emb = self.embed_query(query)
        extracted_preferences = self.extract_preferences_from_response(response)
        for preference in extracted_preferences:
            if preference in preferences:
                query_emb += self.embed_query(preference)
        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)    # !
        start_retrieval = time.time()
        D, I = index.search(query_emb, top_k)
        retrieval_time = time.time() - start_retrieval
        return [chunks[i] for i in I[0]], retrieval_time

    def retrieve_top_k_wq_llm_2(self, query, preferences, index, chunks, top_k=TOP_K):
        """First-Level"""
        system_prompt = self.load_prompt_template(PROMPT_LLM_WQ_CHECK_SYSTEM_PROMPT)
        preferences_text = "\n".join(f"{i + 1}. {p}" for i, p in enumerate(preferences))
        filled_user_prompt = self.load_prompt_template(PROMPT_LLM_WQ_CHECK_USER_PROMPT).format(preferences=preferences_text, question=query)
        response = self.generate_message_vllm(
            messages=[{"role": "user", "content": filled_user_prompt}],
            system_prompt=system_prompt,
            max_tokens=1024
        )

        """Second-Level"""
        if "true" in response.lower():
            system_prompt = self.load_prompt_template(PROMPT_LLM_WQ_SYSTEM_PROMPT)
            filled_user_prompt = self.load_prompt_template(PROMPT_LLM_WQ_USER_PROMPT).format(preferences=preferences_text, question=query)
            response = self.generate_message_vllm(
                messages=[{"role": "user", "content": filled_user_prompt}],
                system_prompt=system_prompt,
                max_tokens=1024
            )
        
        query_emb = self.embed_query(query)
        extracted_preferences = self.extract_preferences_from_response(response)
        for preference in extracted_preferences:
            if preference in preferences:
                query_emb += self.embed_query(preference)
        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)    # !
        start_retrieval = time.time()
        D, I = index.search(query_emb, top_k)
        retrieval_time = time.time() - start_retrieval
        return [chunks[i] for i in I[0]], retrieval_time

    def load_prompt_template(self, file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
            
    def format_prompt(self, template, preference_text, chunk_text):
        return template.replace("{preference}", preference_text).replace("{chunk}", chunk_text)

    def save_jsonl(self, file_path, items):
        with open(file_path, 'a', encoding='utf-8') as f:
            for item in items:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    def save_json(self, file_path, data):
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def load_json(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def save_csv(self, file_path, fieldnames, row, write_header=False):
        write_header = write_header or not os.path.exists(file_path)
        with open(file_path, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            if write_header:
                writer.writeheader()
            writer.writerow(row)

    def parse_decision_and_reason(self, input_string):
        """LLM ì‘ë‹µì—ì„œ decisionê³¼ reason íŒŒì‹±"""
        soup = BeautifulSoup(input_string, "html.parser")
        decision_tag = soup.find("decision")
        reason_tag = soup.find("reason")
        decision = decision_tag.text.strip() if decision_tag else ""
        reason = reason_tag.text.strip() if reason_tag else ""
        return decision, reason

    def parse_decision_and_reason_preference(self, input_string):
        """LLM ì‘ë‹µì—ì„œ decisionê³¼ reason, preference íŒŒì‹±"""
        soup = BeautifulSoup(input_string, "html.parser")
        decision_tag = soup.find("decision")
        reason_tag = soup.find("reason")
        preference_tag = soup.find("relevant_preference")
        decision = decision_tag.text.strip() if decision_tag else ""
        reason = reason_tag.text.strip() if reason_tag else ""
        preference = preference_tag.text.strip() if preference_tag else ""
        
        # preference í…ìŠ¤íŠ¸ ì •ë¦¬
        preference = self.clean_preference_text(preference)
        
        return decision, reason, preference

    def clean_preference_text(self, preference_text):
        """ë‹¤ì–‘í•œ í˜•íƒœì˜ preference í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì„ í˜¸ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        if not preference_text:
            return ""
        
        # ì—¬ëŸ¬ ì¤„ë¡œ ë˜ì–´ ìˆëŠ” ê²½ìš° í•œ ì¤„ë¡œ ë§Œë“¤ê¸°
        preference_text = preference_text.strip()
        
        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "1, 2" ë˜ëŠ” "5")
        if preference_text.replace(",", "").replace(" ", "").isdigit():
            return preference_text
        
        # "Preference X:" í˜•íƒœ ì œê±°
        import re
        preference_text = re.sub(r'^Preference\s+\d+:\s*', '', preference_text, flags=re.IGNORECASE)
        
        # ì•ì— ìˆ«ìì™€ ì ì´ ìˆëŠ” ê²½ìš° ì œê±° (ì˜ˆ: "1. I prefer...")
        preference_text = re.sub(r'^\d+\.\s*', '', preference_text)
        
        # ë”°ì˜´í‘œ ì œê±°
        preference_text = preference_text.strip('"\'')
        
        return preference_text.strip()

    def map_preference_numbers_to_text(self, preference_text, preference_list):
        """ìˆ«ìë¡œ ëœ preferenceë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘"""
        if not preference_text or not preference_list:
            return preference_text
        
        import re
        
        # ìˆ«ìë“¤ì„ ì°¾ì•„ì„œ ì‹¤ì œ preference í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘
        # "1, 2" -> [1, 2] ë˜ëŠ” "5" -> [5]
        numbers = re.findall(r'\d+', preference_text)
        
        if not numbers:
            return preference_text
        
        # ìˆ«ìë¥¼ ì‹¤ì œ preference í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        mapped_preferences = []
        for num in numbers:
            try:
                index = int(num) - 1  # 1-based indexë¥¼ 0-basedë¡œ ë³€í™˜
                if 0 <= index < len(preference_list):
                    mapped_preferences.append(preference_list[index])
                else:
                    # ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì›ë˜ ìˆ«ì ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    mapped_preferences.append(num)
            except ValueError:
                # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                mapped_preferences.append(num)
        
        if mapped_preferences:
            return "; ".join(mapped_preferences)
        else:
            return preference_text

    def parse_summary(self, input_string):
        """LLM ì‘ë‹µì—ì„œ summary íŒŒì‹±"""
        soup = BeautifulSoup(input_string, "html.parser")
        summary_tag = soup.find("summary")
        if summary_tag:
            return summary_tag.text.strip()
        else:
            # summary íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° fallback ì²˜ë¦¬
            text = input_string.strip()
            
            # íŒ¨í„´ 1: "Based on the provided user preferences..." ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
            if text.startswith("Based on the provided user preferences"):
                # "Document Chunk:" ë˜ëŠ” "Chunk:" ì´í›„ì˜ ì‹¤ì œ ë‚´ìš© ì°¾ê¸°
                chunk_match = re.search(r"(?:Document )?Chunk:\s*(.+?)(?:\n\n|$)", text, re.DOTALL)
                if chunk_match:
                    # ì°¾ì€ ì²­í¬ ë‚´ìš©ì„ ë°˜í™˜ (\n\nì—ì„œ ëŠì–´ì§)
                    chunk_content = chunk_match.group(1).strip()
                    return chunk_content
                else:
                    # "Document Chunk:" ë˜ëŠ” "Chunk:"ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë¬¸ì¥ìœ¼ë¡œ ë°˜í™˜    
                    return text
            
            # ê·¸ ì™¸ì˜ ê²½ìš° ì „ì²´ ì‘ë‹µ ë°˜í™˜ (ì§§ì€ ê²½ìš°)
            return None

    def process_chunk(self, chunk_text, preference_text, prompt_template, prompt_template_system=None, preference_list=None):
        filled_prompt = self.format_prompt(prompt_template, preference_text, chunk_text)
        try:
            if prompt_template_system is None:
                llm_response = self.generate_message_vllm(
                    messages=[{"role": "user", "content": filled_prompt}],
                    system_prompt="You are a helpful assistant for indexing document chunks."
                )
                decision, reason = self.parse_decision_and_reason(llm_response)
            else:
                llm_response = self.generate_message_vllm(
                    messages=[{"role": "user", "content": filled_prompt}],
                    system_prompt=prompt_template_system
                )
                decision, reason, preference = self.parse_decision_and_reason_preference(llm_response)
                
                # ìˆ«ìë¡œ ëœ preferenceë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘
                if preference_list and preference:
                    preference = self.map_preference_numbers_to_text(preference, preference_list)
            
            if decision == "":
                return None
            if prompt_template_system is None:
                return {
                    "chunk": chunk_text,
                    "decision": decision,
                    "reason": reason,
                    "status": "success"
                }
            else:
                return {
                    "chunk": chunk_text,
                    "decision": decision,
                    "reason": reason,
                    "relevant_preference": preference,
                    "status": "success"
                }
        except Exception as e:
            print(f"Failed to process chunk: {e}")
            return {
                "chunk": chunk_text,
                "decision": "Filter",  # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ í•„í„°
                "reason": f"LLM processing failed: {str(e)}",
                "status": "failed"
            }

    def process_chunk_per_preference(self, chunk_text, preference_list, prompt_template):
        """ê° ì„ í˜¸ë„ë³„ë¡œ ê°œë³„ì ìœ¼ë¡œ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ì¢…í•©"""
        per_preference_results = []
        
        # ê° ì„ í˜¸ë„ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
        for i, preference in enumerate(preference_list):
            preference_text = f"- {preference}"
            filled_prompt = self.format_prompt(prompt_template, preference_text, chunk_text)
            
            try:
                llm_response = self.generate_message_vllm(
                    messages=[{"role": "user", "content": filled_prompt}],
                    system_prompt="You are a helpful assistant for indexing document chunks."
                )
                decision, reason = self.parse_decision_and_reason(llm_response)
                
                per_preference_results.append({
                    "preference_index": i,
                    "preference": preference,
                    "decision": decision if decision else "Filter",  # ë¹ˆ ì‘ë‹µì€ Filterë¡œ ì²˜ë¦¬
                    "reason": reason
                })
            except Exception as e:
                print(f"Failed to process chunk for preference {i}: {e}")
                per_preference_results.append({
                    "preference_index": i, 
                    "preference": preference,
                    "decision": "Filter",
                    "reason": f"LLM processing failed: {str(e)}"
                })
        
        # ê²°ê³¼ ì¢…í•© ë° ìµœì¢… ê²°ì •
        summarize_preferences = []
        keep_preferences = []
        filter_count = 0
        
        for result in per_preference_results:
            if result["decision"] == "Summarize":
                summarize_preferences.append(result["preference"])
            elif result["decision"] == "Keep As-Is":
                keep_preferences.append(result["preference"])
            elif result["decision"] == "Filter":
                filter_count += 1
        
        # ìµœì¢… ê²°ì • ë¡œì§
        if summarize_preferences:
            # Summarizeê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ Summarize (í•´ë‹¹ ì„ í˜¸ë„ë“¤ì„ ëª¨ì•„ì„œ ì‚¬ìš©)
            final_decision = "Summarize"
            relevant_preferences = summarize_preferences
            final_reason = f"Relevant to {len(summarize_preferences)} preference(s): {', '.join(summarize_preferences[:2])}{'...' if len(summarize_preferences) > 2 else ''}"
        elif keep_preferences:
            # Summarizeê°€ ì—†ê³  Keepì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ Keep
            final_decision = "Keep As-Is"
            relevant_preferences = keep_preferences
            final_reason = f"Directly relevant to {len(keep_preferences)} preference(s): {', '.join(keep_preferences[:2])}{'...' if len(keep_preferences) > 2 else ''}"
        else:
            # ëª¨ë‘ Filterì´ë©´ Filter
            final_decision = "Filter"
            relevant_preferences = []
            final_reason = "Not relevant to any user preferences"
        
        return {
            "chunk": chunk_text,
            "decision": final_decision,
            "reason": final_reason,
            "relevant_preferences": relevant_preferences,  # Summarizeë‚˜ Keepì— ê´€ë ¨ëœ ì„ í˜¸ë„ë“¤
            "per_preference_results": per_preference_results,  # ê° ì„ í˜¸ë„ë³„ ìƒì„¸ ê²°ê³¼
            "status": "success"
        }

    def process_chunk_with_resume(self, chunk_text, preference_text, prompt_template, prompt_template_system, preference_list, result_file, chunk_index):
        """
        ì‹¤ì‹œê°„ ì €ì¥ ê¸°ëŠ¥ì´ ìˆëŠ” ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜ (JSONL í˜•ì‹ìœ¼ë¡œ append)
        """
        # ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ì¸ì§€ í™•ì¸
        if os.path.exists(result_file):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            result = json.loads(line.strip())
                            if result.get("chunk_index") == chunk_index or result["chunk"] == chunk_text:
                                return result
            except (json.JSONDecodeError, FileNotFoundError):
                pass

        # ìƒˆë¡œìš´ ì²­í¬ ì²˜ë¦¬
        filled_prompt = self.format_prompt(prompt_template, preference_text, chunk_text)
        try:
            llm_response = self.generate_message_vllm(
                messages=[{"role": "user", "content": filled_prompt}],
                system_prompt=prompt_template_system
            )
            decision, reason, preference = self.parse_decision_and_reason_preference(llm_response)
            
            # ìˆ«ìë¡œ ëœ preferenceë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘
            if preference_list and preference:
                preference = self.map_preference_numbers_to_text(preference, preference_list)
            
            if decision == "":
                return None
            
            result = {
                "chunk": chunk_text,
                "chunk_index": chunk_index,
                "decision": decision,
                "reason": reason,
                "relevant_preference": preference,
                "status": "success"
            }
            
            # JSONL í˜•ì‹ìœ¼ë¡œ íŒŒì¼ ëì— ì¶”ê°€ (ë©€í‹°ìŠ¤ë ˆë”© ì•ˆì „)
            with open(result_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
            
            return result
            
        except Exception as e:
            print(f"Failed to process chunk {chunk_index}: {e}")
            result = {
                "chunk": chunk_text,
                "chunk_index": chunk_index,
                "decision": "Filter",  # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ í•„í„°
                "reason": f"LLM processing failed: {str(e)}",
                "status": "failed"
            }
            
            # ì‹¤íŒ¨í•œ ê²½ìš°ë„ JSONL í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            with open(result_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
            
            return result

    def load_existing_results_with_resume(self, result_file):
        """
        ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ ë¡œë“œ (JSONL í˜•ì‹)
        """
        if os.path.exists(result_file):
            try:
                existing_results = []
                with open(result_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            existing_results.append(json.loads(line.strip()))
                return existing_results
            except (json.JSONDecodeError, FileNotFoundError):
                return []
        return []

    def summarize_single(self, entry, summarizing_prompt, preference="N/A", summarizing_prompt_system=None):
        # entryê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
        if isinstance(entry, dict):
            original_chunk = entry["chunk"]
            reason = entry.get("reason", "")
            if self.method == "per_pref":
                preference = entry.get("relevant_preferences", [])
                preference_text = "\n".join([f"- {p}" for p in preference])
            else:
                preference_text = preference
            
            if summarizing_prompt_system is None:
                filled_prompt = summarizing_prompt.replace("{preference}", preference_text).replace("{chunk}", original_chunk).replace("{reason}", reason)
                llm_response = self.generate_message_vllm(
                    messages=[{"role": "user", "content": filled_prompt}],
                    system_prompt="You are a helpful assistant tasked with summarizing document chunks."
                )
            else:
                preference_text = entry.get("relevant_preference", [])
                filled_prompt = summarizing_prompt.replace("{preference}", preference_text).replace("{chunk}", original_chunk).replace("{reason}", reason)
                llm_response = self.generate_message_vllm(
                    messages=[{"role": "user", "content": filled_prompt}],
                    system_prompt=summarizing_prompt_system
                )
            # <summary> íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ
            summarized_text = self.parse_summary(llm_response)
            if summarized_text is None:
                return {
                    "summarized": original_chunk,
                    "original": original_chunk,
                    "reason": reason
                }
            elif summarizing_prompt_system is not None:
                return {
                    "summarized": summarized_text,
                    "original": original_chunk,
                    "reason": reason,
                    "relevant_preference": preference_text
                }
            else:
                return {
                    "summarized": summarized_text,
                    "original": original_chunk,
                    "reason": reason
                }
        else:
            # entryê°€ ë¬¸ìì—´ì¸ ê²½ìš° (kept_chunksì—ì„œ ì˜¨ ê²½ìš°)
            original_chunk = entry
            if preference == "N/A":
                filled_prompt = summarizing_prompt.replace("{chunk}", original_chunk)
            else:
                filled_prompt = summarizing_prompt.replace("{preference}", preference).replace("{chunk}", original_chunk)
            llm_response = self.generate_message_vllm(
                messages=[{"role": "user", "content": filled_prompt}],
                system_prompt="You are a helpful assistant tasked with summarizing document chunks."
            )
            # <summary> íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ
            summarized_text = self.parse_summary(llm_response)
            if summarized_text is None:
                return {
                    "summarized": original_chunk,
                    "original": original_chunk
                }
            else:
                return {
                    "summarized": summarized_text,
                    "original": original_chunk
                }


        
    def extract_likes_dislikes(self, preference):
        prompt = f"""
Given the following preference statement, extract the likes and dislikes in a structured format.

Preference: {preference}

Please respond in the following format:
<likes>
A clear, concise sentence describing what the person likes or prefers
</likes>
<dislikes>
A clear, concise sentence describing what the person dislikes or prefers less. If there are no dislikes mentioned, write "None"
</dislikes>

Note:
- If the preference uses "more interested in X than Y", X goes in likes and Y in dislikes
- If the preference uses "less interested in X than Y", X goes in dislikes and Y in likes
- Express likes and dislikes as complete, meaningful sentences
- Be specific and concise in your extraction
- If there are multiple likes/dislikes, combine them into coherent sentences
- If no dislikes are mentioned in the preference, explicitly write "None" in the dislikes section
"""
        # LLM í˜¸ì¶œ ë° ê²°ê³¼ íŒŒì‹±
        response = self.generate_message_vllm(
                messages=[{"role": "user", "content": prompt}],
                system_prompt="You are a helpful assistant tasked with extracting likes and dislikes from preference statements."
            )

        # ê²°ê³¼ íŒŒì‹±
        likes = re.search(r"<likes>(.*?)</likes>", response, re.DOTALL)
        dislikes = re.search(r"<dislikes>(.*?)</dislikes>", response, re.DOTALL)
        
        return {
            "likes": likes.group(1).strip() if likes else "",
            "dislikes": dislikes.group(1).strip() if dislikes else "None"
        }

    def parse_numbered_preferences(self, preference_text, preference_list):
        """
        ë²ˆí˜¸ê°€ í¬í•¨ëœ ì„ í˜¸ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì‹¤ì œ ì„ í˜¸ í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­
        
        Args:
            preference_text: ë²ˆí˜¸ê°€ í¬í•¨ëœ ì„ í˜¸ í…ìŠ¤íŠ¸ (ì˜ˆ: "1. I am fascinated by Renaissance...\n5. I love visiting heritage sites...")
            preference_list: ì›ë³¸ ì„ í˜¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            matched_preferences: ë§¤ì¹­ëœ ì„ í˜¸ í…ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        matched_preferences = []
        
        # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        lines = preference_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ë²ˆí˜¸ê°€ í¬í•¨ëœ í˜•íƒœì¸ì§€ í™•ì¸ (ì˜ˆ: "1. ", "2. ", "5. ")
            if line[0].isdigit() and '. ' in line:
                # ë²ˆí˜¸ì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬
                dot_index = line.find('. ')
                if dot_index != -1:
                    extracted_text = line[dot_index + 2:].strip()
                    
                    # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ preference_listì—ì„œ ì°¾ê¸°
                    for pref in preference_list:
                        if extracted_text == pref:
                            matched_preferences.append(pref)
                            break
                    else:
                        # ì™„ì „ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
                        for pref in preference_list:
                            if extracted_text in pref or pref in extracted_text:
                                matched_preferences.append(pref)
                                break
                        else:
                            print(f"Warning: Could not match preference text: '{extracted_text}'")
            else:
                # ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ë§¤ì¹­
                for pref in preference_list:
                    if line == pref:
                        matched_preferences.append(pref)
                        break
                else:
                    # ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
                    for pref in preference_list:
                        if line in pref or pref in line:
                            matched_preferences.append(pref)
                            break
                    else:
                        print(f"Warning: Could not match preference text: '{line}'")
        
        return matched_preferences

    def enhance_embeddings_with_preferences(self, embeddings, preference_embeddings, document_preference_mapping, preference_list):
        """
        ë¬¸ì„œ ì„ë² ë”©ì— ì„ í˜¸ ì„ë² ë”©ì„ ë”í•´ì„œ ë°©í–¥ì„ ì¡°ì •
        
        Args:
            embeddings: ë¬¸ì„œ ì„ë² ë”© (numpy array)
            preference_embeddings: ì„ í˜¸ ì„ë² ë”© (numpy array)
            document_preference_mapping: ë¬¸ì„œë³„ ê´€ë ¨ ì„ í˜¸ ë§¤í•‘ ì •ë³´ (list of dicts)
            preference_list: ì„ í˜¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            enhanced_embeddings: í–¥ìƒëœ ì„ë² ë”© (numpy array)
        """
        print(f"Enhancing embeddings with preferences...")
        enhanced_embeddings = embeddings.copy()
        
        # ì„ í˜¸ í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
        preference_to_idx = {pref: idx for idx, pref in enumerate(preference_list)}
        
        for i, doc_info in enumerate(document_preference_mapping):
            if 'relevant_preference' in doc_info and doc_info['relevant_preference']:
                relevant_prefs_raw = doc_info['relevant_preference']
                
                # ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                if isinstance(relevant_prefs_raw, str):
                    # ë²ˆí˜¸ê°€ í¬í•¨ëœ í˜•íƒœì¸ì§€ í™•ì¸
                    if '\n' in relevant_prefs_raw and any(line.strip()[0].isdigit() for line in relevant_prefs_raw.split('\n') if line.strip()):
                        # ë²ˆí˜¸ê°€ í¬í•¨ëœ ê²½ìš° íŒŒì‹±
                        relevant_prefs = self.parse_numbered_preferences(relevant_prefs_raw, preference_list)
                    else:
                        # ë‹¨ì¼ ì„ í˜¸ì¸ ê²½ìš°
                        relevant_prefs = [relevant_prefs_raw]
                else:
                    # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    relevant_prefs = relevant_prefs_raw
                
                # ê´€ë ¨ ì„ í˜¸ ì„ë² ë”©ë“¤ ìˆ˜ì§‘
                pref_embeddings = []
                for pref in relevant_prefs:
                    if pref in preference_to_idx:
                        pref_idx = preference_to_idx[pref]
                        pref_embeddings.append(preference_embeddings[pref_idx])
                    else:
                        print(f"Warning: Preference '{pref}' not found in preference_list")
                
                # ì„ í˜¸ ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš° í‰ê· ì„ êµ¬í•´ì„œ ë”í•˜ê¸°
                if pref_embeddings:
                    pref_embeddings = np.array(pref_embeddings)
                    # ì—¬ëŸ¬ ì„ í˜¸ê°€ ìˆëŠ” ê²½ìš° í‰ê· ì„ êµ¬í•¨
                    avg_pref_embedding = np.mean(pref_embeddings, axis=0)
                    # ì •ê·œí™”
                    avg_pref_embedding = avg_pref_embedding / np.linalg.norm(avg_pref_embedding)
                    
                    # ë¬¸ì„œ ì„ë² ë”©ì— ì„ í˜¸ ì„ë² ë”©ì„ ë”í•˜ê¸°
                    enhanced_embeddings[i] = embeddings[i] + avg_pref_embedding
                    # ë‹¤ì‹œ ì •ê·œí™”
                    enhanced_embeddings[i] = enhanced_embeddings[i] / np.linalg.norm(enhanced_embeddings[i])
        
        print(f"Enhanced {len([d for d in document_preference_mapping if 'relevant_preference' in d and d['relevant_preference']])} embeddings out of {len(document_preference_mapping)}")
        return enhanced_embeddings
