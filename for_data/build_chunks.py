import os
import re
import json
import random
from tqdm import tqdm
import argparse

def load_wiki_documents(file_path):
    """wiki ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  id, title, text ì •ë³´ë¥¼ ìœ ì§€"""
    all_docs = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                doc = json.loads(line)
                if doc.get("text", "").strip():
                    all_docs.append(doc)  # ì›ë³¸ ë¬¸ì„œ ê·¸ëŒ€ë¡œ ìœ ì§€
            except json.JSONDecodeError:
                continue
    return all_docs

def load_chunks(file_path):
    """ì´ë¯¸ ìƒì„±ëœ ì²­í¬ë“¤ì„ ë¡œë“œ"""
    chunks = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                chunk = json.loads(line)
                if chunk.get("text", "").strip():
                    chunks.append(chunk)
            except json.JSONDecodeError:
                continue
    return chunks

def clean_text(text):
    # &lt;...&gt; í˜•íƒœì˜ íƒœê·¸ ì œê±°
    text = re.sub(r'&lt;.*?&gt;', '', text)
    return text.strip()

def chunk_documents_sentencewise(docs, chunk_size=100):
    """ë¬¸ì„œë“¤ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œê³ , 100ë‹¨ì–´ ì´í•˜ë¡œ ë¬¶ì–´ì„œ chunk ìƒì„± (ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° í¬í•¨)"""
    chunks = []
    for doc in docs:
        # 1. ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        clean_doc_text = clean_text(doc["text"])
        # 2. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸° (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ, ì¤„ë°”ê¿ˆ ë“±)
        sentences = re.split(r'(?<=[.!?]) +', clean_doc_text)
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            words = sentence.split()
            n_words = len(words)
            if n_words == 0:
                continue

            # ë§Œì•½ í•œ ë¬¸ì¥ì´ chunk_sizeë³´ë‹¤ ê¸¸ë©´, ë‹¨ë… chunkë¡œ
            if n_words > chunk_size:
                if current_chunk:
                    chunks.append({
                        "id": doc["id"],
                        "title": doc["title"],
                        "text": " ".join(current_chunk)
                    })
                    current_chunk = []
                    current_length = 0
                chunks.append({
                    "id": doc["id"],
                    "title": doc["title"],
                    "text": sentence
                })
                continue
            # chunkì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ chunk_sizeë¥¼ ë„˜ìœ¼ë©´, ìƒˆ chunk ì‹œì‘
            if current_length + n_words > chunk_size:
                chunks.append({
                    "id": doc["id"],
                    "title": doc["title"],
                    "text": " ".join(current_chunk)
                })
                current_chunk = []
                current_length = 0

            current_chunk.append(sentence)
            current_length += n_words

        # ë§ˆì§€ë§‰ chunk ì²˜ë¦¬
        if current_chunk:
            chunks.append({
                "id": doc["id"],
                "title": doc["title"],
                "text": " ".join(current_chunk)
            })

    return chunks
    
def chunk_documents(docs, chunk_size=100):
    """ë¬¸ì„œë¥¼ 100ë‹¨ì–´ ì²­í¬ë¡œ ë‚˜ëˆ„ë˜ idì™€ title ì •ë³´ ìœ ì§€"""
    chunks = []
    for doc in docs:
        words = doc["text"].split()
        current_chunk = []
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i+chunk_size]
            current_chunk.extend(chunk_words)
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆê³ , í˜„ì¬ ì²­í¬ê°€ chunk_sizeì— ë„ë‹¬í–ˆì„ ë•Œ
            if i + chunk_size < len(words) and len(current_chunk) >= chunk_size:
                chunk = ' '.join(current_chunk)
                chunks.append({
                    "id": doc["id"],
                    "title": doc["title"],
                    "text": chunk
                })
                current_chunk = []
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk:
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ 20ë‹¨ì–´ ë¯¸ë§Œì´ê³  ì´ì „ ì²­í¬ê°€ ìˆìœ¼ë©´ í•©ì¹˜ê¸°
            if len(current_chunk) < 20 and chunks and chunks[-1]["id"] == doc["id"]:
                chunks[-1]["text"] = chunks[-1]["text"] + " " + ' '.join(current_chunk)
            else:
                chunk = ' '.join(current_chunk)
                chunks.append({
                    "id": doc["id"],
                    "title": doc["title"],
                    "text": chunk
                })
    
    return chunks

def sample_chunks_for_baseline(chunks, sample_ratio=0.01):
    """ê¸°ì¡´ ì²­í¬ë“¤ì—ì„œ ëœë¤ ìƒ˜í”Œë§"""
    random.seed(42)  # ê³ ì •ëœ ì‹œë“œ ì‚¬ìš©
    sampled_chunks = []
    
    for chunk in chunks:
        if random.random() < sample_ratio:
            sampled_chunks.append(chunk)
    
    return sampled_chunks

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build chunks from wiki documents")
    parser.add_argument("--doc_mode", type=str, required=True, 
                        choices=["total", "sample", "related", "related2", "sample_sw", "total_sw", "baseline_sampling"],
                        help="Document mode: 'total' for all documents, 'sample' for sampled documents, 'related' for sampled related documents, 'baseline_sampling' for sampling from existing chunks")
    parser.add_argument("--sample_ratio", type=float, default=0.01,
                        help="Sampling ratio for baseline_sampling mode (default: 0.01 = 1%)")
    args = parser.parse_args()

    if args.doc_mode == "baseline_sampling":
        # baseline_sampling ëª¨ë“œ: ê¸°ì¡´ ì²­í¬ì—ì„œ ìƒ˜í”Œë§
        input_file = "../data/corpus/full_chunks_with_doc_sw.jsonl"
        
        # ìƒ˜í”Œë§ ë¹„ìœ¨ì— ë”°ë¼ íŒŒì¼ëª… ê²°ì •
        if args.sample_ratio == 0.001:  # 0.1%
            output_file = "../data/corpus/forbaseline_sampled_chunks_with_doc_sw_01.jsonl"
        else:  # 1% (ê¸°ë³¸ê°’)
            output_file = "../data/corpus/forbaseline_sampled_chunks_with_doc_sw.jsonl"
        
        print(f"ğŸ“‚ Loading existing chunks from {input_file}...")
        chunks = load_chunks(input_file)
        print(f"âœ… Loaded {len(chunks)} chunks")
        
        print(f"ğŸ² Sampling {args.sample_ratio*100}% of chunks (seed=42)...")
        sampled_chunks = sample_chunks_for_baseline(chunks, args.sample_ratio)
        print(f"âœ… Sampled {len(sampled_chunks)} chunks")
        
        # JSONLë¡œ ì €ì¥
        with open(output_file, "w", encoding="utf-8") as f:
            for chunk in sampled_chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + "\n")
        print(f"ğŸ“„ Sampled chunks saved to: {output_file}")
        print(f"ğŸ“Š Original chunks: {len(chunks)}, Sampled chunks: {len(sampled_chunks)}")
        
    else:
        # ê¸°ì¡´ ëª¨ë“œë“¤
        if args.doc_mode in ["total", "total_sw"]:
            input_file = "../data/corpus/full_wiki_doc.jsonl"
        elif args.doc_mode == "related":
            input_file = "../data/corpus/sampled_related_wiki_doc.jsonl"
        elif args.doc_mode == "related2":
            input_file = "../data/corpus/sampled_related_wiki_doc2.jsonl"
        else:
            input_file = "../data/corpus/sampled_wiki_doc.jsonl"
        output_folder = "../data/corpus"
        os.makedirs(output_folder, exist_ok=True)

        # 1. ë¬¸ì„œ ë¡œë“œ
        print("ğŸ“‚ Loading wiki documents...")
        docs = load_wiki_documents(input_file)
        print(f"âœ… Loaded {len(docs)} documents")

        # 2. ì²­í¬ ìƒì„±
        if args.doc_mode in ["sample_sw", "total_sw"]:
            print("ğŸ§© Splitting into 100-word sentencewise chunks...")
            chunks = chunk_documents_sentencewise(docs, chunk_size=100)
        else:
            print("ğŸ§© Splitting into 100-word chunks...")
            chunks = chunk_documents(docs, chunk_size=100)
        print(f"âœ… Generated {len(chunks)} chunks")

        # 3. JSONLë¡œ ì €ì¥
        if args.doc_mode == "total":
            output_file = os.path.join(output_folder, "full_chunks_with_doc.jsonl")
        elif args.doc_mode == "related":
            output_file = os.path.join(output_folder, "sampled_related_chunks_with_doc.jsonl")
        elif args.doc_mode == "related2":
            output_file = os.path.join(output_folder, "sampled_related_chunks_with_doc2.jsonl")
        elif args.doc_mode == "sample_sw":
            output_file = os.path.join(output_folder, "sampled_chunks_with_doc_sw.jsonl")
        elif args.doc_mode == "total_sw":
            output_file = os.path.join(output_folder, "full_chunks_with_doc_sw.jsonl")
        else:
            output_file = os.path.join(output_folder, "sampled_chunks_with_doc.jsonl")
            
        with open(output_file, "w", encoding="utf-8") as f:
            for chunk in chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + "\n")
        print(f"ğŸ“„ Chunks saved to: {output_file}")
        print(f"ğŸ“Š Total chunks: {len(chunks)}")